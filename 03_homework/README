## **ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì‹¶ì€ë°?**

1. **ê¸€ ìš”ì•½** : 
    
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê¸´ ê¸€ì„ ë¶„ì„í•œ í›„, ì¤‘ìš”í•œ ì •ë³´ë‚˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½, 
    
    ì´ ê³¼ì •ì—ì„œ ì˜ë¯¸ë¥¼ ìƒì§€ ì•Šìœ¼ë©´ì„œë„ ì „ì²´ ê¸€ì˜ ë‚´ìš©ì„ ì••ì¶•
    
2. **ìš”ì•½ëœ ê¸€ ë²ˆì—­** : 
    
    ìš”ì•½ëœ ê¸€ì„ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê³¼ì •, 
    
    ë²ˆì—­ ê¸°ëŠ¥ì€ ë²ˆì—­ì˜ ì •í™•ì„±ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ í•µì‹¬,
    
     ì´ë¡œ ì¸í•´ ì‚¬ìš©ìëŠ” ê¸´ ê¸€ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìš”ì•½í•œ í›„, ê·¸ ìš”ì•½ë³¸ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì‰½ê²Œ ë²ˆì—­
    
3. **ì–¸ì–´ì— ë”°ë¥¸ ì²˜ë¦¬** :
    
    ì–¸ì–´ì— ë”°ë¼ ì²˜ë¦¬í•˜ëŠ” ai ë¥¼ ë¶„ë¦¬
    

## **ë¬¸ì œë¥¼ í•´ê²° í•  ìˆ˜ ìˆëŠ” ai task ì •ë¦¬**

- **facebook/bart-large-cnn - ìš”ì•½**
- **facebook/nllb-200-distilled-600M - ë²ˆì—­**
- **t5-base-korean-summarization - ìš”ì•½(í•œê¸€)**

## **task ë³„ ëª¨ë¸ë“¤ì„ ê°€ì ¸ì™€ ì í•©í•œ ëª¨ë¸ì°¾ê¸°**

- **facebook/bart-large-cnn** ìš”ì•½ / facebook ( ì˜ì–´ ì§€ì› )
    - ì˜ì–´ë¡œ ì‘ì„±ëœ ê¸€
    - **ì˜ˆì œ )**
        
        ```python
        from transformers import pipeline
        import requests
        
        # ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì •ì˜
        pipe = pipeline("summarization", model="facebook/bart-large-cnn")
        
        # API ì„¤ì •
        API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
        headers = {"Authorization": f"Bearer {token}"}
        
        # ì¿¼ë¦¬ í•¨ìˆ˜ ì •ì˜
        def query(payload):
            response = requests.post(API_URL, headers=headers, json=payload)
            return response.json()
        
        # ìš”ì•½í•  í…ìŠ¤íŠ¸ ì…ë ¥ (ê¸¸ì´ë¥¼ ì¤„ì—¬ë´…ë‹ˆë‹¤)
        output = query({
            "inputs": "The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It is 324 metres tall and was the tallest man-made structure for 41 years.",
        })
        
        # ì¶œë ¥ ê²°ê³¼ í™•ì¸
        print("Summary Output:", output)
        
        # ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì •ì˜
        translator = pipeline(
            'translation',
            model='facebook/nllb-200-distilled-600M',
            device=-1,  # CPUì—ì„œ ì‹¤í–‰
            src_lang='eng_Latn',
            tgt_lang='kor_Hang',
            max_length=512
        )
        
        # ë²ˆì—­í•  í…ìŠ¤íŠ¸ ì„¤ì •
        text_to_translate = output[0]['summary_text'] if isinstance(output, list) and len(output) > 0 else ""
        
        # ë²ˆì—­ ìˆ˜í–‰ (max_length ì¡°ì •)
        translated_text = translator(text_to_translate, max_length=256)[0]['translation_text'] if text_to_translate else "No text to translate."
        print("Translated Text:", translated_text)
        ```
        
- **eenzeenee/t5-base-korean-summarization** ìš”ì•½ / ( í•œê¸€ ì§€ì› )
    - í•œê¸€ë¡œ ì‘ì„±ëœ ê¸€
    - **ì˜ˆì œ )**
        
        ```python
        import re
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        import nltk
        
        # NLTKì—ì„œ punkt ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆì„ ê²½ìš° ê±´ë„ˆë›°ê¸°)
        nltk.download('punkt')
        
        # ë°ì´í„° ê²½ë¡œ í™•ì¸
        print(nltk.data.path)  # ë°ì´í„° ê²½ë¡œ ì¶œë ¥
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization')
        tokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization')
        
        prefix = "summarize: "
        sample = """
            ì•ˆë…•í•˜ì„¸ìš”? ìš°ë¦¬ (2í•™ë…„)/(ì´ í•™ë…„) ì¹œêµ¬ë“¤ ìš°ë¦¬ ì¹œêµ¬ë“¤ í•™êµì— ê°€ì„œ ì§„ì§œ (2í•™ë…„)/(ì´ í•™ë…„) ì´ ë˜ê³  ì‹¶ì—ˆëŠ”ë° í•™êµì— ëª» ê°€ê³  ìˆì–´ì„œ ë‹µë‹µí•˜ì£ ? 
            ê·¸ë˜ë„ ìš°ë¦¬ ì¹œêµ¬ë“¤ì˜ ì•ˆì „ê³¼ ê±´ê°•ì´ ìµœìš°ì„ ì´ë‹ˆê¹Œìš” ì˜¤ëŠ˜ë¶€í„° ì„ ìƒë‹˜ì´ë‘ ë§¤ì¼ ë§¤ì¼ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ë³´ë„ë¡ í•´ìš”. 
            ì–´/ ì‹œê°„ì´ ë²Œì¨ ì´ë ‡ê²Œ ëë‚˜ìš”? ëŠ¦ì—ˆì–´ìš”. ëŠ¦ì—ˆì–´ìš”. ë¹¨ë¦¬ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ì•¼ ë¼ìš”. 
            ê·¸ëŸ°ë° ì–´/ êµ­ì–´ì—¬í–‰ì„ ë– ë‚˜ê¸° ì „ì— ìš°ë¦¬ê°€ ì¤€ë¹„ë¬¼ì„ ì±™ê²¨ì•¼ ë˜ê² ì£ ? êµ­ì–´ ì—¬í–‰ì„ ë– ë‚  ì¤€ë¹„ë¬¼, êµì•ˆì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ ì„ ìƒë‹˜ì´ ì„¤ëª…ì„ í•´ì¤„ê²Œìš”. 
            (EBS)/(ì´ë¹„ì—ìŠ¤) ì´ˆë“±ì„ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ê°€ë©´ìš” ì²«í™”ë©´ì´ ì´ë ‡ê²Œ ë‚˜ì™€ìš”. 
            ì/ ê·¸ëŸ¬ë©´ìš” ì—¬ê¸° (X)/(ì—‘ìŠ¤) ëˆŒëŸ¬ì£¼(ê³ ìš”)/(êµ¬ìš”). ì €ê¸° (ë™ê·¸ë¼ë¯¸)/(ë˜¥ê·¸ë¼ë¯¸) (EBS)/(ì´ë¹„ì—ìŠ¤) (2ì£¼)/(ì´ ì£¼) ë¼ì´ë¸ŒíŠ¹ê°•ì´ë¼ê³  ë˜ì–´ìˆì£ ? 
            ê±°ê¸°ë¥¼ ë°”ë¡œ ê°€ê¸°ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤. ì/ (ëˆ„ë¥´ë©´ìš”)/(ëˆŒë¥´ë©´ìš”). ì–´ë–»ê²Œ ë˜ëƒ? b/ ë°‘ìœ¼ë¡œ ë‚´ë ¤ìš” ë‚´ë ¤ìš” ë‚´ë ¤ìš” ì­‰ ë‚´ë ¤ìš”. 
            ìš°ë¦¬ ëª‡ í•™ë…„ì´ì£ ? ì•„/ (2í•™ë…„)/(ì´ í•™ë…„) ì´ì£  (2í•™ë…„)/(ì´ í•™ë…„)ì˜ ë¬´ìŠ¨ ê³¼ëª©? êµ­ì–´. 
            ì´ë²ˆì£¼ëŠ” (1ì£¼)/(ì¼ ì£¼) ì°¨ë‹ˆê¹Œìš” ì—¬ê¸° êµì•ˆ. ë‹¤ìŒì£¼ëŠ” ì—¬ê¸°ì„œ ë‹¤ìš´ì„ ë°›ìœ¼ë©´ ë¼ìš”. 
            ì´ êµì•ˆì„ í´ë¦­ì„ í•˜ë©´, ì§œì”/. ì´ë ‡ê²Œ êµì¬ê°€ ë‚˜ì˜µë‹ˆë‹¤ .ì´ êµì•ˆì„ (ë‹¤ìš´)/(ë”°ìš´)ë°›ì•„ì„œ ìš°ë¦¬ êµ­ì–´ì—¬í–‰ì„ ë– ë‚  ìˆ˜ê°€ ìˆì–´ìš”. 
            ê·¸ëŸ¼ ìš°ë¦¬ ì§„ì§œë¡œ êµ­ì–´ ì—¬í–‰ì„ í•œë²ˆ ë– ë‚˜ë³´ë„ë¡ í•´ìš”? êµ­ì–´ì—¬í–‰ ì¶œë°œ. ì/ (1ë‹¨ì›)/(ì¼ ë‹¨ì›) ì œëª©ì´ ë­”ê°€ìš”? í•œë²ˆ ì°¾ì•„ë´ìš”. 
            ì‹œë¥¼ ì¦ê²¨ìš” ì—ìš”. ê·¸ëƒ¥ ì‹œë¥¼ ì½ì–´ìš” ê°€ ì•„ë‹ˆì—ìš”. ì‹œë¥¼ ì¦ê²¨ì•¼ ë¼ìš” ì¦ê²¨ì•¼ ë¼. ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œ? ì¼ë‹¨ì€ ë‚´ë‚´ ì‹œë¥¼ ì¦ê¸°ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ê³µë¶€ë¥¼ í•  ê±´ë°ìš”. 
            ê·¸ëŸ¼ ì˜¤ëŠ˜ì€ìš” ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œìš”? ì˜¤ëŠ˜ ê³µë¶€í•  ë‚´ìš©ì€ìš” ì‹œë¥¼ ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ê¸°ë¥¼ ê³µë¶€í• ê²ë‹ˆë‹¤. 
            ì–´ë–»ê²Œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ì„ê¹Œ ìš°ë¦¬ ê³µë¶€í•´ ë³´ë„ë¡ í•´ìš”. ì˜¤ëŠ˜ì˜ ì‹œ ë‚˜ì™€ë¼ ì§œì”/! ì‹œê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤ ì‹œì˜ ì œëª©ì´ ë­”ê°€ìš”? ë‹¤íˆ° ë‚ ì´ì—ìš” ë‹¤íˆ° ë‚ . 
            ëˆ„êµ¬ë‘ ë‹¤í‰œë‚˜ ë™ìƒì´ë‘ ë‹¤í‰œë‚˜ ì–¸ë‹ˆë‘ ì¹œêµ¬ë‘? ëˆ„êµ¬ë‘ ë‹¤í‰œëŠ”ì§€ ì„ ìƒë‹˜ì´ ì‹œë¥¼ ì½ì–´ ì¤„ í…Œë‹ˆê¹Œ í•œë²ˆ ìƒê°ì„ í•´ë³´ë„ë¡ í•´ìš”."""
        
        inputs = [prefix + sample]
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors="pt")
        
        # ìš”ì•½ ìƒì„±
        output = model.generate(**inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)
        
        # ë””ì½”ë”©
        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]
        
        # ë¬¸ì¥ ë¶„ë¦¬ ë° ì²« ë¬¸ì¥ ì¶œë ¥ (ì •ê·œ í‘œí˜„ì‹ ì‚¬ìš©)
        result = re.split(r'(?<=[.!?]) +', decoded_output.strip())[0]  # ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        print('RESULT >>', result)
        
        RESULT >> êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ê¸° ì „ì— êµ­ì–´ ì—¬í–‰ì„ ë– ë‚  ì¤€ë¹„ë¬¼ê³¼ êµì•ˆì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ ì„ ìƒë‹˜ì´ ì„¤ëª…í•´ ì¤€ë‹¤.
        ```
        

## í•œê¸€ / ì˜ì–´ ë²ˆì—­ ìš”ì•½ API

[**ğŸªê³¼ì œ_github**](https://github.com/kimkinghyeon/18_DEEP_LEARNING/tree/main/03_homework)

```
from fastapi import FastAPI, HTTPException
from transformers import pipeline
import requests
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re
# import nltk : nltk ì„¤ì¹˜ê°€ í•„ìš”í• ë•Œ ì‚¬ìš© [ì´ë¯¸ ë‹¤ìš´ë°›ì•„ì„œ ê±´ë„ˆë›°ê¸°]

app = FastAPI()

# API ì„¤ì •
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
token = ""  # ìì‹ ì˜ í† í°ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”
headers = {"Authorization": f"Bearer {token}"}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    if response.status_code != 200:
        raise HTTPException(status_code=response.status_code, detail=response.json())
    return response.json()

# ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì •ì˜
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ì •ì˜
translator = pipeline(
    'translation',
    model='facebook/nllb-200-distilled-600M',
    device=-1,  # CPUì—ì„œ ì‹¤í–‰
    src_lang='eng_Latn',
    tgt_lang='kor_Hang',
    max_length=512
)

@app.post("/en-summarize/")
async def summarize(text: str):
    # ìš”ì•½ ìš”ì²­
    output = query({"inputs": text})
    summary_text = output[0]['summary_text'] if isinstance(output, list) and len(output) > 0 else ""

    # ë²ˆì—­ ìš”ì²­
    translated_text = translator(summary_text, max_length=256)[0]['translation_text'] if summary_text else "No text to translate."

    return {"summary": summary_text, "translated_summary": translated_text}

# í•œêµ­ì–´ ìš”ì•½ì„ ìœ„í•œ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = 'eenzeenee/t5-base-korean-summarization'
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prefix = "summarize: "

@app.post("/kr-summarize/")
async def kr_summarize(text: str):
    # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    inputs = [prefix + text]

    # í…ìŠ¤íŠ¸ í† í°í™”
    tokenized_inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors="pt")

    # ìš”ì•½ ìƒì„±
    output = model.generate(**tokenized_inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)

    # ë””ì½”ë”©
    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]

    # ë¬¸ì¥ ë¶„ë¦¬ ë° ì²« ë¬¸ì¥ ì¶œë ¥
    result = re.split(r'(?<=[.!?]) +', decoded_output.strip())[0]  # ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    return {"summary": result}

```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/2e42b292-3597-492a-9d2f-caaf0ff36a48/48cc73fd-1367-49d8-9e48-ee14b12b04ad/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/2e42b292-3597-492a-9d2f-caaf0ff36a48/ae4971f8-8a8b-40fd-8f29-415876d5642e/image.png)
