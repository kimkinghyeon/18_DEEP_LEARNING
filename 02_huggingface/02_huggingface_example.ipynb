{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8964897394180298}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 모델 가져오기\n",
    "classfier = pipeline('text-classification')\n",
    "\n",
    "# 문제\n",
    "text = \"I've been wating for a HuggingFace course my whole life\"\n",
    "\n",
    "classfier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8964897394180298},\n",
       " {'label': 'NEGATIVE', 'score': 0.9980971217155457}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 감정분석\n",
    "classfier = pipeline('sentiment-analysis')\n",
    "\n",
    "classfier([\n",
    "    \"I've been wating for a HuggingFace course my whole life\",\n",
    "    \"I hate tihs so much!\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 제로샷 분류\n",
    "* Zero-shot : 기계 학습 및 자연어 처리에서 각 개별 작업에 대한 특정요육 없이 작업을 수행할 수 있는 모델 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This course provides valuable insights into the principles and applications of machine learning.',\n",
       " 'labels': ['home', 'education', 'business'],\n",
       " 'scores': [0.6292843222618103, 0.23532681167125702, 0.13538892567157745]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# squence_to_classify = 'one day I will see the world'\n",
    "# squence_to_classify2 = 'This a course about the transformers library'\n",
    "squence_to_classify = \"This course provides valuable insights into the principles and applications of machine learning.\"\n",
    "\n",
    "# candidate_lavbels = ['travel','cooking','dancing']\n",
    "# candidate_lavbels2 = ['cook','business','education']\n",
    "candidate_lavbels = ['home','business','education']\n",
    "\n",
    "# classifier(squence_to_classify,candidate_lavbels)\n",
    "# classifier(squence_to_classify2,candidate_lavbels2)\n",
    "classifier(squence_to_classify,candidate_lavbels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 생성 (Text Generation)\n",
    "* 주어진 텍스트 데이터를 기반으로 새로운 텍스트를 생성하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'go to home. He may have a very hard time coming up with a name for himself, but we will just have to hope that this season isn\\'t the first time it\\'s been said he\\'s been asked for an audition.\\n\\n\"Last'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation','gpt2')\n",
    "# generator('In this coures, we will teach you how to',pad_token_id = generator.tokenizer.eos_token_id)\n",
    "generator('go to home',pad_token_id = generator.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**마스크 채우기 모델**\n",
    "* 텍스트에서 일부 단어를 마스크로 대체하고 이를 채우는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2060806006193161,\n",
       "  'token': 3299,\n",
       "  'token_str': ' doctor',\n",
       "  'sequence': \"i'm going to hospital and meet a doctor\"},\n",
       " {'score': 0.08788839727640152,\n",
       "  'token': 1441,\n",
       "  'token_str': ' friend',\n",
       "  'sequence': \"i'm going to hospital and meet a friend\"},\n",
       " {'score': 0.06839483231306076,\n",
       "  'token': 9008,\n",
       "  'token_str': ' nurse',\n",
       "  'sequence': \"i'm going to hospital and meet a nurse\"},\n",
       " {'score': 0.051201723515987396,\n",
       "  'token': 27321,\n",
       "  'token_str': ' psychiatrist',\n",
       "  'sequence': \"i'm going to hospital and meet a psychiatrist\"},\n",
       " {'score': 0.04677021503448486,\n",
       "  'token': 16308,\n",
       "  'token_str': ' surgeon',\n",
       "  'sequence': \"i'm going to hospital and meet a surgeon\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask','distilroberta-base')\n",
    "\n",
    "unmasker(\"i'm going to hospital and meet a <mask>\",top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10731075704097748,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i ' m a fashion model.\"},\n",
       " {'score': 0.08774492889642715,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role',\n",
       "  'sequence': \"hello i ' m a role model.\"},\n",
       " {'score': 0.053383972495794296,\n",
       "  'token': 2047,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"hello i ' m a new model.\"},\n",
       " {'score': 0.046672359108924866,\n",
       "  'token': 3565,\n",
       "  'token_str': 'super',\n",
       "  'sequence': \"hello i ' m a super model.\"},\n",
       " {'score': 0.027095895260572433,\n",
       "  'token': 2986,\n",
       "  'token_str': 'fine',\n",
       "  'sequence': \"hello i ' m a fine model.\"}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model = 'bert-base-uncased')\n",
    "\n",
    "unmasker(\"hello i'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05429058149456978,\n",
       "  'token': 81053,\n",
       "  'token_str': '자동차',\n",
       "  'sequence': '안녕하세요? 나는 자동차 모델입니다.'},\n",
       " {'score': 0.03555170074105263,\n",
       "  'token': 56575,\n",
       "  'token_str': 'Fashion',\n",
       "  'sequence': '안녕하세요? 나는 Fashion 모델입니다.'},\n",
       " {'score': 0.030248744413256645,\n",
       "  'token': 53226,\n",
       "  'token_str': '광고',\n",
       "  'sequence': '안녕하세요? 나는 광고 모델입니다.'},\n",
       " {'score': 0.024159586057066917,\n",
       "  'token': 54543,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': '안녕하세요? 나는 fashion 모델입니다.'},\n",
       " {'score': 0.02254818007349968,\n",
       "  'token': 26084,\n",
       "  'token_str': '사진',\n",
       "  'sequence': '안녕하세요? 나는 사진 모델입니다.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "pipe(\"안녕하세요? 나는 <mask> 모델입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", \"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)  # 개체명 인식\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", \"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)  # 개체명 인식\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**개체명 인식**\n",
    "* NER(Name Entity Recognition)\n",
    "* 텍스트에서 사람,위치,조직 등의 객체명을 인식하는 작업\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", \"distilbert-base-cased-distilled-squad\")  # 질의 응답\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949770450592041, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", \"distilbert-base-cased-distilled-squad\")  # 질의 응답\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\",\"sshleifer/distilbart-cnn-12-6\")  # 요약\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "    \n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to configure the Windows Server 2012 R2 Virtual Machine with the latest version of the Windows Server SDK, Windows Update and Windows Server 2008 R2. You will also learn the following: how you can upgrade to'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", \"gpt2\")\n",
    "generator(\"In this course, we will teach you how to\", pad_token_id=generator.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "imagetotext = pipeline(\"image-to-text\", \"ydshieh/vit-gpt2-coco-en\")\n",
    "imagetotext(\"https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "url = 'https://huggingface.co/datasets/Narsil/image_dummy/resolve/main/parrots.png'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "url = 'https://pds.joongang.co.kr/news/component/htmlphoto_mmdata/202306/25/488f9638-800c-4bac-ad65-82877fbff79b.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
